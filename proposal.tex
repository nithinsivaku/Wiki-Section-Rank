\documentclass[conference]{IEEEtran}

\parindent 0pt
\parskip 5pt
\pagestyle{plain}

\title{Text Classification approaches to Complex Retrieval Track}
\author{Nithin Sivakumar}


\newcommand{\namelistlabel}[1]{\mbox{#1}\hfil}
\newenvironment{namelist}[1]{%1
\begin{list}{}
    {
        \let\makelabel\namelistlabel
        \settowidth{\labelwidth}{#1}
        \setlength{\leftmargin}{1.1\labelwidth}
    }
  }{%1
\end{list}}

\begin{document}
\maketitle



 \section*{THESIS STATEMENT}
Given an outline for complex topic outline Q, retrieve for each of its sections Hi, a ranking of relevant passages
S. The passage S is taken from the provided passage corpus. We define a passage as relevant if the
passage content is appropriate for the knowledge article.

\section*{Background} An information retrieval process begins when a user enters a query into the system. Queries are formal statements
of information needs, for example search strings in web search engines. In information retrieval a query does not
uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with
different degrees of relevancy. An object is an entity that is represented by information in a content collection
or database. User queries are matched against the database information. However, as opposed to classical SQL
queries of a database, in information retrieval the results returned may or may not match the query, so results
are typically ranked. This ranking of results is a key difference of information retrieval searching compared
to database searching \cite{manning}. Ranking is the main task to be done for information retrieval. Given a
set of documents, the task is to assign a relevance score to each of them and then sort them based on that
score to have the most relevant documents on top. The applications of ranking include document retrieval,
summary extraction, collaborative filtering, question answering, sentiment analysis, product rating, and many
more. 

\subsection*{Motivation}
Text classification is a smart classification of text into categories. And, using machine learning to automate
these tasks, just makes the whole process super-fast and efficient. Supervised classification of text is done when
you have defined the classification categories. It works on training and testing principle. we feed labeled data
to the machine learning algorithm to work on. The algorithm is trained on the labeled dataset and gives the
desired output(the pre-defined categories). During the testing phase, the algorithm is fed with unobserved data
and classifies them into categories based on the training phase. I apply the same technique to my classification
model. [Sauper2009] \cite{c1} retrieved content from the web on articles belonging
to the category of diseases by using the most frequent section titles as keywords to retrieve relevant web-snippets
(excerpts). The most informative excerpts were selected using a perceptron-based framework and populated into
the Wikipedia article. Banerjee \cite{c2},  \cite{c3},  \cite{c4} proposed WikiKreator where contents
in the Wikipedia sections were represented by topic-distribution features using Latent Dirichlet Allocation (LDA)
\cite{c5}. The features were used to train classifiers and predict sections for new content retrieved from the
web. I propose a framework where a text classifier is built using topic distribution vectors. 
The paper is structured as follows 1) I describe my previous work on TREC CAR dataset \cite{c6} and retrieval models in section III. 2) I describe the dataset, previous results and evaluation metrics I have in section IV. 3) I describe the Approach and methods in my research in section V. 4) I describe the implications of the research in section VI. 5) I talk about conclusion and future work in section VII. 


\section*{Previous work} 
\subsection*{BM25 - Baseline} 
 In information retrieval, Okapi BM25 (BM stands for Best Matching) is a ranking function used by search engines to rank matching documents according to their relevance to a given search query. \cite{c7} BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document. 
Given a query Q, containing keywords q1,...,qn, the BM25 score of a document D is:

\[ score (D, Q) = \sum_{i=1}^{n} IDF (q\textsubscript{i}) . \frac{f(q\textsubscript{i} . D) .(k\textsubscript{1} + 1)}           {f(q\textsubscript{i} . D) + (k\textsubscript{1} . (1 - b + b . \frac{|D|}{avgdl}) } \]


where f(q\textsubscript{i} . D) is q\textsubscript{i}`s term frequency in the document D, D is the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn.



\section*{Experimental Evaluation}
In this section you should outline how you intend to go
about accomplishing the aims you have set in the previous
section. Try to break your grand aims down into small,
achievable tasks. Try to estimate how long you will
spend on each task, and draw up a timetable for each
sub-task.

\section*{Approach}
Outline what your specific requirements will be with regard
to software and hardware, but note that any special requests
might need to be approved by your supervisor and the Head of
Department.

Overall, you should aim to produce roughly a two page document
(and certainly no more than four pages)
outlining your plan for the year.

\section*{Implications of Research}
Outline what your specific requirements will be with regard
to software and hardware, but note that any special requests
might need to be approved by your supervisor and the Head of
Department.

Overall, you should aim to produce roughly a two page document
(and certainly no more than four pages)
outlining your plan for the year.

\begin{thebibliography}{9}
\bibitem{c1} Christina Sauper and Regina Barzilay. Automatically generating wikipedia articles: A structure aware
approach. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume
1, pages 208-216. Association for Computational Linguistics, 2009.
\bibitem{c2} Siddhartha Banerjee and Prasenjit Mitra. Filling the Gaps: Improving Wikipedia Stubs. In
the 15th ACM SIGWEB International Symposium on Document Engineering (DocEng) Laussanne,
Switzerland: ACM.

\bibitem{c3} Siddhartha Banerjee and Prasenjit Mitra. WikiKreator: Improving Wikipedia Stubs Automatically.
In the 53rd Annual Meeting of the Association for Computational Linguistics (ACL). Beijing,
China: ACL.
\bibitem{c4} Siddhartha Banerjee and Prasenjit Mitra. Wikikreator: Improving wikipedia stubs automatically.
IIn Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural Language Processing of the Asian Fed- eration of
Natural Language Processing, ACL 2015, July 26- 31, 2015, Beijing, China, Volume 1: Long Papers,
pages 867- 877, 2015.
\bibitem{c5} David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of
machine Learning research, 3:993-1022, 2003.
Antennas Propagat., to be publised.
\bibitem{c6} Laura Dietz, Ben Gamari. ``TREC CAR 2.0: A Data Set for Complex Answer Retrieval``. Version
2.0, 2018. http://trec-car.cs.unh.edu
\bibitem{c7} Jansen, B. J. and Rieh, S. (2010)  ``The Seventeen Theoretical Constructs of Information Searching
and Information Retrieval.``. Journal of the American Society for Information Sciences and Technology.
61(8), 1517-1534.
\bibitem{manning} Christopher D. Manning, Prabhakar Raghavan, Hinrich Schutze. ``An Introduction to Information Retrieval ``, Cambridge University Press, 2009, p. 233.
\end{thebibliography}


\end{document}

